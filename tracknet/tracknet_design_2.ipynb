{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMmw5kBwBVG8CmN3rLenh9L"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8782423,"sourceType":"datasetVersion","datasetId":5279262}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom tensorboardX import SummaryWriter\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport os\nimport cv2\nimport numpy as np\nimport json\nimport numpy as np\nfrom sympy import Line \nimport sympy\nfrom scipy.spatial import distance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_best_accuracy = 0\nnum_epochs = 25\nsteps_per_epoch = 300\nval_intervals = 20\nbatch_size = 2\nlr = 1e-5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gaussian2D(shape, sigma=1):\n    m, n = [(ss - 1.) / 2. for ss in shape]\n    y, x = np.ogrid[-m:m+1,-n:n+1]\n\n    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n    return h\n\ndef draw_umich_gaussian(heatmap, center, radius, k=1):\n    diameter = 2 * radius + 1\n    gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n\n    x, y = int(center[0]), int(center[1])\n\n    height, width = heatmap.shape[0:2]\n\n    left, right = min(x, radius), min(width - x, radius + 1)\n    top, bottom = min(y, radius), min(height - y, radius + 1)\n\n    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n    masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0: \n        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n    return heatmap\n\n\ndef gaussian_radius(det_size, min_overlap=0.7):\n    height, width = det_size\n\n    a1  = 1\n    b1  = (height + width)\n    c1  = width * height * (1 - min_overlap) / (1 + min_overlap)\n    sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n    r1  = (b1 + sq1) / 2\n\n    a2  = 4\n    b2  = 2 * (height + width)\n    c2  = (1 - min_overlap) * width * height\n    sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)\n    r2  = (b2 + sq2) / 2\n\n    a3  = 4 * min_overlap\n    b3  = -2 * min_overlap * (height + width)\n    c3  = (min_overlap - 1) * width * height\n    sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)\n    r3  = (b3 + sq3) / 2\n    return min(r1, r2, r3)\n\ndef line_intersection(line1, line2):\n    \"\"\"\n    Find 2 lines intersection point\n    \"\"\"\n    l1 = Line((line1[0], line1[1]), (line1[2], line1[3]))\n    l2 = Line((line2[0], line2[1]), (line2[2], line2[3]))\n\n    intersection = l1.intersection(l2)\n    point = None\n    if len(intersection) > 0:\n        if isinstance(intersection[0], sympy.geometry.point.Point2D):\n            point = intersection[0].coordinates\n    return point\n\n\ndef is_point_in_image(x, y, input_width=1280, input_height=720):\n    res = False\n    if x and y:\n        res = (x >= 0) and (x <= input_width) and (y >= 0) and (y <= input_height)\n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=bias),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n            nn.ReLU(),\n            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n        )\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return x * self.sigmoid(out)\n\nclass TrackNet(nn.Module):\n    def __init__(self, input_channels=3, out_channels=14):\n        super().__init__()\n        self.out_channels = out_channels\n        self.input_channels = input_channels\n\n        self.conv1 = ConvBlock(in_channels=self.input_channels, out_channels=64)\n        self.conv2 = ConvBlock(in_channels=64, out_channels=64)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv3 = ConvBlock(in_channels=64, out_channels=128)\n        self.conv4 = ConvBlock(in_channels=128, out_channels=128)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv5 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv6 = ConvBlock(in_channels=256, out_channels=256)\n        self.conv7 = ConvBlock(in_channels=256, out_channels=256)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv8 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv9 = ConvBlock(in_channels=512, out_channels=512)\n        self.conv10 = ConvBlock(in_channels=512, out_channels=512)\n        self.ups1 = nn.Upsample(scale_factor=2)\n        self.conv11 = ConvBlock(in_channels=512, out_channels=256)\n        self.conv12 = ConvBlock(in_channels=256, out_channels=256)\n        self.ca1 = ChannelAttention(in_channels=256)\n        self.ups2 = nn.Upsample(scale_factor=2)\n        self.conv13 = ConvBlock(in_channels=256, out_channels=128)\n        self.conv14 = ConvBlock(in_channels=128, out_channels=128)\n        self.ca2 = ChannelAttention(in_channels=128)\n        self.ups3 = nn.Upsample(scale_factor=2)\n        self.conv15 = ConvBlock(in_channels=128, out_channels=64)\n        self.conv16 = ConvBlock(in_channels=64, out_channels=64)\n        self.ca3 = ChannelAttention(in_channels=64)\n        self.conv17 = ConvBlock(in_channels=64, out_channels=self.out_channels)\n\n        self._init_weights()\n                  \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)   \n        x = self.pool1(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.pool2(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.conv7(x)\n        x = self.pool3(x)\n        x = self.conv8(x)\n        x = self.conv9(x)\n        x = self.conv10(x)\n        x = self.ups1(x)\n        x = self.conv11(x)\n        x = self.conv12(x)\n        x = self.ca1(x)\n        x = self.ups2(x)\n        x = self.conv13(x)\n        x = self.conv14(x)\n        x = self.ca2(x)\n        x = self.ups3(x)\n        x = self.conv15(x)\n        x = self.conv16(x)\n        x = self.ca3(x)\n        x = self.conv17(x)\n        return x\n    \n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Conv2d):\n                nn.init.uniform_(module.weight, -0.05, 0.05)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n\n            elif isinstance(module, nn.BatchNorm2d):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class courtDataset(Dataset):\n    def __init__(self, mode, input_height=720, input_width=1280, scale=2, hp_radius=55):\n        self.mode = mode\n        assert mode in ['train', 'val'], 'incorrect mode'\n        self.input_height = input_height\n        self.input_width = input_width\n        self.output_height = int(input_height/scale)\n        self.output_width = int(input_width/scale)\n        self.num_joints = 14\n        self.hp_radius = hp_radius\n        self.scale = scale\n\n        self.path_dataset = '/kaggle/input/court-keypoints/data'\n        self.path_images = os.path.join(self.path_dataset, 'images')\n        with open(os.path.join(self.path_dataset, 'data_{}.json'.format(mode)), 'r') as f:\n            self.data = json.load(f)\n        print('mode = {}, len = {}'.format(mode, len(self.data)))\n\n\n    def filter_data(self):\n        new_data = []\n        for i in range(len(self.data)):\n            max_elems = np.array(self.data[i]['kps']).max(axis=0)\n            min_elems = np.array(self.data[i]['kps']).min(axis=0)\n            if max_elems[0] < self.input_width and min_elems[0] > 0 and max_elems[1] < self.input_height and \\\n                    min_elems[1] > 0:\n                new_data.append(self.data[i])\n        return new_data\n\n        \n    def __getitem__(self, index):\n        img_name = self.data[index]['id'] + '.png'\n        kps = self.data[index]['kps']\n        img = cv2.imread(os.path.join(self.path_images, img_name))\n        img = cv2.resize(img, (self.output_width, self.output_height))\n        inp = (img.astype(np.float32) / 255.)\n        inp = np.rollaxis(inp, 2, 0)\n\n        hm_hp = np.zeros((self.num_joints+1, self.output_height, self.output_width), dtype=np.float32)\n        draw_gaussian = draw_umich_gaussian\n\n        for i in range(len(kps)):\n            if kps[i][0] >=0 and kps[i][0] <= self.input_width and kps[i][1] >=0 and kps[i][1] <= self.input_height:\n            # if is_point_in_image(kps[i][0], kps[i][1], self.input_width, self.input_height):\n                x_pt_int = int(kps[i][0]/self.scale)\n                y_pt_int = int(kps[i][1]/self.scale)\n                draw_gaussian(hm_hp[i], (x_pt_int, y_pt_int), self.hp_radius)\n\n        # draw center point of tennis court\n        x_ct, y_ct = line_intersection((kps[0][0], kps[0][1], kps[3][0], kps[3][1]), (kps[1][0], kps[1][1],\n                                                                                      kps[2][0], kps[2][1]))\n        draw_gaussian(hm_hp[self.num_joints], (int(x_ct/self.scale), int(y_ct/self.scale)), self.hp_radius)\n        \n        return inp, hm_hp, np.array(kps, dtype=int), img_name[:-4]\n        \n        \n    def __len__(self):\n        return len(self.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess(heatmap, scale=2, low_thresh=155, min_radius=10, max_radius=30):\n    x_pred, y_pred = None, None\n    ret, heatmap = cv2.threshold(heatmap, low_thresh, 255, cv2.THRESH_BINARY)\n    circles = cv2.HoughCircles(heatmap, cv2.HOUGH_GRADIENT, dp=1, minDist=20, param1=50, param2=2, minRadius=min_radius,\n                               maxRadius=max_radius)\n    if circles is not None:\n        x_pred = circles[0][0][0] * scale\n        y_pred = circles[0][0][1] * scale\n    return x_pred, y_pred\n\ndef refine_kps(img, x_ct, y_ct, crop_size=40):\n    refined_x_ct, refined_y_ct = x_ct, y_ct\n    \n    img_height, img_width = img.shape[:2]\n    x_min = max(x_ct-crop_size, 0)\n    x_max = min(img_height, x_ct+crop_size)\n    y_min = max(y_ct-crop_size, 0)\n    y_max = min(img_width, y_ct+crop_size)\n\n    img_crop = img[x_min:x_max, y_min:y_max]\n    lines = detect_lines(img_crop)\n    \n    if len(lines) > 1:\n        lines = merge_lines(lines)\n        if len(lines) == 2:\n            inters = line_intersection(lines[0], lines[1])\n            if inters:\n                new_x_ct = int(inters[1])\n                new_y_ct = int(inters[0])\n                if new_x_ct > 0 and new_x_ct < img_crop.shape[0] and new_y_ct > 0 and new_y_ct < img_crop.shape[1]:\n                    refined_x_ct = x_min + new_x_ct\n                    refined_y_ct = y_min + new_y_ct                    \n    return refined_y_ct, refined_x_ct\n\n\ndef detect_lines(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = cv2.threshold(gray, 155, 255, cv2.THRESH_BINARY)[1]\n    lines = cv2.HoughLinesP(gray, 1, np.pi / 180, 30, minLineLength=10, maxLineGap=30)\n    lines = np.squeeze(lines) \n    if len(lines.shape) > 0:\n        if len(lines) == 4 and not isinstance(lines[0], np.ndarray):\n            lines = [lines]\n    else:\n        lines = []\n    return lines\n\ndef merge_lines(lines):\n    lines = sorted(lines, key=lambda item: item[0])\n    mask = [True] * len(lines)\n    new_lines = []\n\n    for i, line in enumerate(lines):\n        if mask[i]:\n            for j, s_line in enumerate(lines[i + 1:]):\n                if mask[i + j + 1]:\n                    x1, y1, x2, y2 = line\n                    x3, y3, x4, y4 = s_line\n                    dist1 = distance.euclidean((x1, y1), (x3, y3))\n                    dist2 = distance.euclidean((x2, y2), (x4, y4))\n                    if dist1 < 20 and dist2 < 20:\n                        line = np.array([int((x1+x3)/2), int((y1+y3)/2), int((x2+x4)/2), int((y2+y4)/2)],\n                                        dtype=np.int32)\n                        mask[i + j + 1] = False\n            new_lines.append(line)  \n    return new_lines       ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = courtDataset('train')\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=1,\n    pin_memory=True\n)\n\nval_dataset = courtDataset('val')\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=1,\n    pin_memory=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val(model, val_loader, criterion, device, epoch):\n    model.eval()\n    losses = []\n    tp, fp, fn, tn = 0, 0, 0, 0\n    max_dist = 7\n    for iter_id, batch in enumerate(val_loader):\n        with torch.no_grad():\n            batch_size = batch[0].shape[0]\n            out = model(batch[0].float().to(device))\n            kps = batch[2]\n            gt_hm = batch[1].float().to(device)\n            loss = criterion(F.sigmoid(out), gt_hm)\n\n            pred = F.sigmoid(out).detach().cpu().numpy()\n            for bs in range(batch_size):\n                for kps_num in range(14):\n                    heatmap = (pred[bs][kps_num] * 255).astype(np.uint8)\n                    x_pred, y_pred = postprocess(heatmap)\n                    x_gt = kps[bs][kps_num][0].item()\n                    y_gt = kps[bs][kps_num][1].item()\n\n                    if is_point_in_image(x_pred, y_pred) and is_point_in_image(x_gt, y_gt):\n                        dst = distance.euclidean((x_pred, y_pred), (x_gt, y_gt))\n                        if dst < max_dist:\n                            tp += 1\n                        else:\n                            fp += 1\n                    elif is_point_in_image(x_pred, y_pred) and not is_point_in_image(x_gt, y_gt):\n                        fp += 1\n                    elif not is_point_in_image(x_pred, y_pred) and is_point_in_image(x_gt, y_gt):\n                        fn += 1\n                    elif not is_point_in_image(x_pred, y_pred) and not is_point_in_image(x_gt, y_gt):\n                        tn += 1\n\n            eps = 1e-15\n            precision = round(tp / (tp + fp + eps), 5)\n            accuracy = round((tp + tn) / (tp + tn + fp + fn + eps), 5)\n            print('val, epoch = {}, iter_id = {}/{}, loss = {}, tp = {}, fp = {}, fn = {}, tn = {}, precision = {}, '\n                  'accuracy = {}'.format(epoch, iter_id, len(val_loader), round(loss.item(), 5), tp, fp, fn, tn,\n                                         precision, accuracy))\n            losses.append(loss.item())\n    return np.mean(losses), tp, fp, fn, tn, precision, accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TrackNet(out_channels=15).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr, betas=(0.9, 0.999), weight_decay=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, train_loader, optimizer, criterion, device, epoch, max_iters=1000):\n    model.train()\n    losses = []\n    max_iters = min(max_iters, len(train_loader))\n\n    for iter_id, batch in enumerate(train_loader):\n        out = model(batch[0].float().to(device))\n        gt_hm_hp = batch[1].float().to(device)\n        loss = criterion(F.sigmoid(out), gt_hm_hp)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        print('train, epoch = {}, iter_id = {}/{}, loss = {}'.format(epoch, iter_id, max_iters, loss.item()))\n        losses.append(loss.item())\n        if iter_id > max_iters:\n            break\n\n    return np.mean(losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_id = 'default'\nexps_path = './exps/{}'.format(exp_id)\ntb_path = os.path.join(exps_path, 'plots')\nif not os.path.exists(tb_path):\n    os.makedirs(tb_path)\nlog_writer = SummaryWriter(tb_path)\nmodel_last_path = os.path.join(exps_path, 'model_last.pt')\nmodel_best_path = os.path.join(exps_path, 'model_best.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    train_loss = train(model, train_loader, optimizer, criterion, device, epoch, steps_per_epoch)\n    log_writer.add_scalar('Train/training_loss', train_loss, epoch)\n\n    if (epoch > 0) & (epoch % val_intervals == 0):\n        val_loss, tp, fp, fn, tn, precision, accuracy = val(model, val_loader, criterion, device, epoch)\n        print('val loss = {}'.format(val_loss))\n        log_writer.add_scalar('Val/loss', val_loss, epoch)\n        log_writer.add_scalar('Val/tp', tp, epoch)\n        log_writer.add_scalar('Val/fp', fp, epoch)\n        log_writer.add_scalar('Val/fn', fn, epoch)\n        log_writer.add_scalar('Val/tn', tn, epoch)\n        log_writer.add_scalar('Val/precision', precision, epoch)\n        log_writer.add_scalar('Val/accuracy', accuracy, epoch)\n        if accuracy > val_best_accuracy:\n            val_best_accuracy = accuracy\n            torch.save(model.state_dict(), model_best_path)     \n        torch.save(model.state_dict(), model_last_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}