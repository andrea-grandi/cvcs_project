{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNxK9jQhxVr268lP/7seMvc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZ5QRJCaCDWv","executionInfo":{"status":"ok","timestamp":1719310141898,"user_tz":-120,"elapsed":2490,"user":{"displayName":"ANDREA GRANDI","userId":"16345933086319672875"}},"outputId":"72e98602-f2be-44c2-a291-3a0002918b41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","zip_filename = '/content/drive/MyDrive/Datasets/tennis_court_det_dataset.zip'\n","\n","with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n","    zip_ref.extractall('./')"],"metadata":{"id":"J2EBosMhCFYs","executionInfo":{"status":"error","timestamp":1719314402869,"user_tz":-120,"elapsed":1688,"user":{"displayName":"ANDREA GRANDI","userId":"16345933086319672875"}},"colab":{"base_uri":"https://localhost:8080/","height":337},"outputId":"91ecaf68-c6a5-464a-d591-87bf44a8eca2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1efba6e39946>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import json\n","import os\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","\n","class TennisCourtDataset(Dataset):\n","    def __init__(self, json_file, img_dir, transform=None):\n","        self.annotations = json.load(open(json_file))\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        img_id = self.annotations[idx]['id']\n","        img_path = os.path.join(self.img_dir, img_id + '.png')\n","        image = Image.open(img_path).convert(\"RGB\")\n","        keypoints = self.annotations[idx]['kps']\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n","        return image, keypoints\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_json = '/content/data/data/data_train.json'\n","val_json = '/content/data/data/data_val.json'\n","img_dir = '/content/data/data/images/'\n","\n","train_dataset = TennisCourtDataset(json_file=train_json, img_dir=img_dir, transform=transform)\n","val_dataset = TennisCourtDataset(json_file=val_json, img_dir=img_dir, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)"],"metadata":{"id":"5ZuWFM-ICIjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_83Wb7XBxCg"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=bias),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","class UpConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=pad, bias=bias),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","class ChannelAttention(nn.Module):\n","    def __init__(self, in_channels, ratio=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","\n","        self.fc= nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels // ratio, kernel_size=1, padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels // ratio, in_channels, kernel_size=1, padding=0, bias=False)\n","        )\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avgout = self.fc(self.avg_pool(x))\n","        maxout = self.fc(self.max_pool(x))\n","        return self.sigmoid(avgout + maxout)\n","\n","\n","class TrackNet(nn.Module):\n","    def __init__(self, input_channels=3, out_channels=14):\n","        super().__init__()\n","        self.out_channels = out_channels\n","        self.input_channels = input_channels\n","\n","        self.conv1 = ConvBlock(in_channels=self.input_channels, out_channels=64)\n","        self.conv2 = ConvBlock(in_channels=64, out_channels=64)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv3 = ConvBlock(in_channels=64, out_channels=128)\n","        self.conv4 = ConvBlock(in_channels=128, out_channels=128)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv5 = ConvBlock(in_channels=128, out_channels=256)\n","        self.conv6 = ConvBlock(in_channels=256, out_channels=256)\n","        self.conv7 = ConvBlock(in_channels=256, out_channels=256)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv8 = ConvBlock(in_channels=256, out_channels=512)\n","        self.conv9 = ConvBlock(in_channels=512, out_channels=512)\n","        self.conv10 = ConvBlock(in_channels=512, out_channels=512)\n","\n","        self.upconv1 = UpConvBlock(in_channels=512, out_channels=256, kernel_size=2, stride=2, pad=0)\n","        self.conv11 = ConvBlock(in_channels=512, out_channels=256)\n","        self.conv12 = ConvBlock(in_channels=256, out_channels=256)\n","        self.conv13 = ConvBlock(in_channels=256, out_channels=256)\n","        self.attention1 = ChannelAttention(256)\n","\n","        self.upconv2 = UpConvBlock(in_channels=256, out_channels=128, kernel_size=2, stride=2, pad=0)\n","        self.conv14 = ConvBlock(in_channels=256, out_channels=128)\n","        self.conv15 = ConvBlock(in_channels=128, out_channels=128)\n","        self.attention2 = ChannelAttention(128)\n","\n","        self.upconv3 = UpConvBlock(in_channels=128, out_channels=64, kernel_size=2, stride=2, pad=0)\n","        self.conv16 = ConvBlock(in_channels=128, out_channels=64)\n","        self.conv17 = ConvBlock(in_channels=64, out_channels=64)\n","        self.attention3 = ChannelAttention(64)\n","        self.conv18 = nn.Conv2d(in_channels=64, out_channels=self.out_channels, kernel_size=1, stride=1, padding=0)\n","\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((2, 2))\n","        self.flatten = nn.Flatten()\n","        self.fc = nn.Linear(self.out_channels * 2 * 2, 2 * 14)\n","        self._init_weights()\n","\n","    def forward(self, x):\n","        c1 = self.conv1(x)\n","        c2 = self.conv2(c1)\n","        p1 = self.pool1(c2)\n","\n","        c3 = self.conv3(p1)\n","        c4 = self.conv4(c3)\n","        p2 = self.pool2(c4)\n","\n","        c5 = self.conv5(p2)\n","        c6 = self.conv6(c5)\n","        c7 = self.conv7(c6)\n","        p3 = self.pool3(c7)\n","\n","        c8 = self.conv8(p3)\n","        c9 = self.conv9(c8)\n","        c10 = self.conv10(c9)\n","\n","        u1 = self.upconv1(c10)\n","        cat1 = torch.cat([u1, c7], dim=1)\n","        c11 = self.conv11(cat1)\n","        c12 = self.conv12(c11)\n","        c13 = self.conv13(c12)\n","        a1 = self.attention1(c13)\n","\n","        u2 = self.upconv2(a1)\n","        cat2 = torch.cat([u2, c4], dim=1)\n","        c14 = self.conv14(cat2)\n","        c15 = self.conv15(c14)\n","        a2 = self.attention2(c15)\n","\n","        u3 = self.upconv3(a2)\n","        cat3 = torch.cat([u3, c2], dim=1)\n","        c16 = self.conv16(cat3)\n","        c17 = self.conv17(c16)\n","        c18 = self.conv18(c17)\n","        a3 = self.attention3(c18)\n","\n","        pooled = self.adaptive_pool(a3)\n","        flattened = self.flatten(pooled)\n","        out = self.fc(flattened)\n","        out = out.view(-1, self.out_channels, 2)\n","\n","        return out\n","\n","    def _init_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d):\n","                nn.init.uniform_(module.weight, -0.05, 0.05)\n","                if module.bias is not None:\n","                    nn.init.constant_(module.bias, 0)\n","\n","            elif isinstance(module, nn.BatchNorm2d):\n","                nn.init.constant_(module.weight, 1)\n","                nn.init.constant_(module.bias, 0)"]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TrackNet().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"7CFAYmDhCXRW","executionInfo":{"status":"ok","timestamp":1719312438128,"user_tz":-120,"elapsed":608,"user":{"displayName":"ANDREA GRANDI","userId":"16345933086319672875"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"57e02bfb-795e-472c-bad3-2978a47e6f27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["\"\"\"\n","num_epochs = 20\n","best_val_loss = float('inf')\n","\n","for epoch in range(num_epochs):\n","    print(f'Epoch: {epoch}')\n","    model.train()\n","    running_loss = 0.0\n","    for images, keypoints in train_loader:\n","        images = images.to(device)\n","        keypoints = keypoints.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","\n","        loss = criterion(outputs, keypoints)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, keypoints in val_loader:\n","            images = images.to(device)\n","            keypoints = keypoints.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, keypoints)\n","            val_loss += loss.item() * images.size(0)\n","\n","    val_loss /= len(val_loader.dataset)\n","    print(f'Validation Loss: {val_loss:.4f}')\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_model.pth')\n","        print(f'Modello salvato con Validation Loss migliorato: {best_val_loss:.4f}')\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":239},"id":"ATIy9DmRxLpc","executionInfo":{"status":"error","timestamp":1719312453463,"user_tz":-120,"elapsed":14492,"user":{"displayName":"ANDREA GRANDI","userId":"16345933086319672875"}},"outputId":"c9b6ecfc-c292-4dfb-e3b8-e5851e1a9f9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-c7bc378c52f9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_dir):\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch + 1}/{num_epochs}')\n","        model.train()\n","        running_loss = 0.0\n","\n","        for i, (images, targets) in enumerate(train_loader):\n","            images = images.to(device)\n","            targets = targets.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            if i % 100 == 99:\n","                average_loss = running_loss / 100\n","                print(f'[{epoch + 1}, {i + 1:4d}] loss: {average_loss:.4f}')\n","                running_loss = 0.0\n","\n","        val_loss = evaluate_model(model, val_loader, criterion, device)\n","        print(f'Epoch {epoch + 1} validation loss: {val_loss:.4f}')\n","\n","        torch.save(model.state_dict(), f'{save_dir}/model_epoch_{epoch + 1}.pth')\n","\n","def evaluate_model(model, loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","\n","    with torch.no_grad():\n","        for i, (images, targets) in enumerate(loader):\n","            images = images.to(device)\n","            targets = targets.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            running_loss += loss.item()\n","\n","    val_loss = running_loss / len(loader)\n","    return val_loss\n","\n","save_dir = '/content/'\n","num_epochs = 20\n","\n","train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_dir)"],"metadata":{"id":"QYhs1K4mytxt"},"execution_count":null,"outputs":[]}]}